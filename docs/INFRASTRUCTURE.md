# Infrastructure - Documentation complÃ¨te

## Vue d'ensemble

L'infrastructure DevBoard est dÃ©ployÃ©e sur **Proxmox** avec **Terraform** pour l'IaC, **Ansible** pour la configuration, et **K3s** pour l'orchestration Kubernetes.

---

## Architecture Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Proxmox Host                       â”‚
â”‚                   (proxade)                          â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  LXC 400     â”‚  â”‚  LXC 401     â”‚  â”‚  LXC 402     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  â”‚ K3s Server   â”‚  â”‚ K3s Agent 1  â”‚  â”‚ K3s Agent 2  â”‚
â”‚  â”‚ (Master)     â”‚  â”‚ (Worker)     â”‚  â”‚ (Worker)     â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  â”‚ 192.168.1.40 â”‚  â”‚ 192.168.1.41 â”‚  â”‚ 192.168.1.42 â”‚
â”‚  â”‚ 4 CPU        â”‚  â”‚ 4 CPU        â”‚  â”‚ 4 CPU        â”‚
â”‚  â”‚ 4096 MB RAM  â”‚  â”‚ 3072 MB RAM  â”‚  â”‚ 3072 MB RAM  â”‚
â”‚  â”‚ 32 GB Disk   â”‚  â”‚ 32 GB Disk   â”‚  â”‚ 32 GB Disk   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Terraform - Provisioning des LXC

### ğŸ“ Emplacement
`infra/terraform/`

### ğŸ¯ RÃ´le
CrÃ©e les containers LXC sur Proxmox via l'API.

### ğŸ“‹ Ressources crÃ©Ã©es

| VMID | Nom         | RÃ´le        | IP           | CPU | RAM  | Disk |
|------|-------------|-------------|--------------|-----|------|------|
| 400  | k3s-server  | Control Plane | 192.168.1.40 | 4   | 4GB  | 32GB |
| 401  | k3s-agent-1 | Worker      | 192.168.1.41 | 4   | 3GB  | 32GB |
| 402  | k3s-agent-2 | Worker      | 192.168.1.42 | 4   | 3GB  | 32GB |

### âš™ï¸ Configuration LXC pour K3s

Les containers nÃ©cessitent des configurations spÃ©cifiques pour exÃ©cuter K3s :

```conf
# Configuration LXC (/etc/pve/lxc/400.conf)
unprivileged: 0                          # Mode privileged requis
features: fuse=1,keyctl=1,mknod=1,nesting=1
lxc.mount.auto: proc:rw sys:rw cgroup:rw # /proc et /sys en Ã©criture
lxc.apparmor.profile: unconfined         # DÃ©sactiver apparmor
lxc.cap.drop:                            # Garder toutes les capabilities
```

**Fixes critiques appliquÃ©s** :
- `/dev/kmsg` : `ln -sf /dev/console /dev/kmsg` (requis par kubelet)
- `iptables` : Installation requise pour le rÃ©seau K3s
- Procfs/Sysfs RW : Requis pour les paramÃ¨tres kernel

### ğŸš€ Commandes Terraform

```bash
cd infra/terraform

# Initialiser Terraform
terraform init

# Planifier les changements
terraform plan

# Appliquer la configuration
terraform apply

# DÃ©truire l'infrastructure
terraform destroy
```

### ğŸ”§ Variables Terraform

Les variables sont dans `variables.tf` :
- `proxmox_api_url` : URL de l'API Proxmox
- `proxmox_token_id` : ID du token API
- `proxmox_token_secret` : Secret du token
- `target_node` : Nom du nÅ“ud Proxmox cible

---

## 2. Ansible - Configuration et dÃ©ploiement

### ğŸ“ Emplacement
`infra/ansible/`

### ğŸ¯ RÃ´le
Configure les LXC et dÃ©ploie K3s + stack DevOps (Prometheus, Grafana, Loki, Vault).

### ğŸ“‚ Structure

```
infra/ansible/
â”œâ”€â”€ inventory/
â”‚   â””â”€â”€ dev.yml              # Inventaire des hosts
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ install-k3s.yml      # Installation K3s
â”‚   â””â”€â”€ deploy-tools.yml     # DÃ©ploiement des outils
â””â”€â”€ roles/
    â”œâ”€â”€ k3s-server/
    â”œâ”€â”€ k3s-agent/
    â”œâ”€â”€ monitoring/
    â””â”€â”€ vault/
```

### ğŸ“‹ Inventaire (`inventory/dev.yml`)

```yaml
all:
  vars:
    ansible_user: root
    ansible_ssh_private_key_file: ~/.ssh/id_rsa
    k3s_version: v1.31.4+k3s1

  children:
    k3s_server:
      hosts:
        k3s-server:
          ansible_host: 192.168.1.40

    k3s_agents:
      hosts:
        k3s-agent-1:
          ansible_host: 192.168.1.41
        k3s-agent-2:
          ansible_host: 192.168.1.42
```

### ğŸ“œ Playbook 1 : Installation K3s

**Fichier** : `playbooks/install-k3s.yml`

**Ã‰tapes** :
1. Installation des prÃ©requis (curl, iptables, apparmor)
2. Installation du server K3s sur le nÅ“ud 400
3. RÃ©cupÃ©ration du token K3s
4. TÃ©lÃ©chargement du kubeconfig
5. Installation des agents K3s sur les nÅ“uds 401-402

**ExÃ©cution** :
```bash
cd infra/ansible
ansible-playbook -i inventory/dev.yml playbooks/install-k3s.yml
```

**RÃ©sultat** :
- Cluster K3s opÃ©rationnel (1 master + 2 workers)
- Kubeconfig disponible : `infra/ansible/kubeconfig.yaml`

### ğŸ“œ Playbook 2 : DÃ©ploiement des outils

**Fichier** : `playbooks/deploy-tools.yml`

**Ã‰tapes** :
1. Installation de Helm
2. Ajout des repos Helm (prometheus-community, grafana, hashicorp)
3. CrÃ©ation des namespaces (devboard-*, monitoring, security)
4. Installation de kube-prometheus-stack (Prometheus + Grafana)
5. Installation de Loki-stack (Loki + Promtail)
6. Installation de Vault (mode dev)

**ExÃ©cution** :
```bash
cd infra/ansible
ansible-playbook -i inventory/dev.yml playbooks/deploy-tools.yml
```

**RÃ©sultat** :
- Prometheus + Grafana dÃ©ployÃ©s (namespace `monitoring`)
- Loki + Promtail dÃ©ployÃ©s (logs centralisÃ©s)
- Vault dÃ©ployÃ© (namespace `security`)

### ğŸ”‘ AccÃ¨s SSH aux nÅ“uds

```bash
# Server K3s
ssh root@192.168.1.40

# Agents K3s
ssh root@192.168.1.41
ssh root@192.168.1.42
```

---

## 3. K3s - Cluster Kubernetes

### ğŸ¯ CaractÃ©ristiques

- **Version** : v1.31.4+k3s1
- **Distribution** : K3s (Kubernetes lÃ©ger)
- **Container Runtime** : containerd 1.7.23
- **Ingress Controller** : Traefik (inclus)
- **Storage** : local-path-provisioner (inclus)
- **Metrics** : metrics-server (inclus)

### ğŸ—ï¸ Architecture du cluster

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            K3s Server (192.168.1.40)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Control Plane                             â”‚  â”‚
â”‚  â”‚  - API Server                              â”‚  â”‚
â”‚  â”‚  - Scheduler                               â”‚  â”‚
â”‚  â”‚  - Controller Manager                      â”‚  â”‚
â”‚  â”‚  - etcd (embedded)                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Worker Components                         â”‚  â”‚
â”‚  â”‚  - kubelet                                 â”‚  â”‚
â”‚  â”‚  - containerd                              â”‚  â”‚
â”‚  â”‚  - Traefik Ingress Controller              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  K3s Agent 1   â”‚       â”‚  K3s Agent 2   â”‚
â”‚ (192.168.1.41) â”‚       â”‚ (192.168.1.42) â”‚
â”‚                â”‚       â”‚                â”‚
â”‚  - kubelet     â”‚       â”‚  - kubelet     â”‚
â”‚  - containerd  â”‚       â”‚  - containerd  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Ã‰tat du cluster

```bash
# VÃ©rifier les nÅ“uds
export KUBECONFIG=/home/tom/Dev/projet-etude/infra/ansible/kubeconfig.yaml
kubectl get nodes -o wide

# RÃ©sultat attendu :
# NAME          STATUS   ROLES                  AGE   VERSION
# k3s-server    Ready    control-plane,master   XXm   v1.31.4+k3s1
# k3s-agent-1   Ready    <none>                 XXm   v1.31.4+k3s1
# k3s-agent-2   Ready    <none>                 XXm   v1.31.4+k3s1
```

### ğŸ”§ AccÃ¨s au cluster

#### Option 1 : Depuis ta machine locale
```bash
export KUBECONFIG=/home/tom/Dev/projet-etude/infra/ansible/kubeconfig.yaml
kubectl get pods -A
```

#### Option 2 : Depuis le serveur K3s
```bash
ssh root@192.168.1.40
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get pods -A
```

### ğŸ“¦ Composants systÃ¨me (namespace kube-system)

| Composant                | RÃ´le                                    |
|-------------------------|-----------------------------------------|
| coredns                 | DNS interne du cluster                  |
| traefik                 | Ingress controller (routing HTTP/HTTPS) |
| metrics-server          | MÃ©triques CPU/RAM des pods              |
| local-path-provisioner  | Dynamic volume provisioning             |
| svclb-traefik-*         | Service LoadBalancer pour Traefik       |

---

## 4. RÃ©seau et Ingress

### ğŸŒ RÃ©seau interne

- **CNI** : Flannel (inclus dans K3s)
- **Pod CIDR** : 10.42.0.0/16
- **Service CIDR** : 10.43.0.0/16

### ğŸšª Ingress Controller : Traefik

Traefik Ã©coute sur le port 80 de **toutes** les IPs du cluster via un Service LoadBalancer.

**Principe** : Virtual Hosting basÃ© sur le header HTTP `Host:`

```yaml
# Exemple d'Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
spec:
  rules:
    - host: grafana.devboard.local
      http:
        paths:
          - backend:
              service:
                name: prometheus-grafana
                port: 80
```

**AccÃ¨s** : http://grafana.devboard.local (aprÃ¨s configuration `/etc/hosts`)

Voir [INGRESS-ROUTING-EXPLAINED.md](INGRESS-ROUTING-EXPLAINED.md) pour les dÃ©tails.

---

## 5. DÃ©pannage

### ProblÃ¨me : K3s ne dÃ©marre pas

**SymptÃ´me** : `open /dev/kmsg: no such file or directory`

**Solution** :
```bash
ssh root@192.168.1.40
ln -sf /dev/console /dev/kmsg
systemctl restart k3s
```

### ProblÃ¨me : "Failed to start ContainerManager" / "read-only file system"

**Cause** : /proc/sys en lecture seule dans le LXC

**Solution** : Ajouter dans `/etc/pve/lxc/400.conf` :
```
lxc.mount.auto: proc:rw sys:rw cgroup:rw
```

Puis redÃ©marrer le container :
```bash
pct stop 400 && pct start 400
```

### ProblÃ¨me : Pods en ImagePullBackOff

**Cause** : Images locales mais imagePullPolicy = Always

**Solution** : DÃ©finir `imagePullPolicy: Never` dans les manifests/Helm values.

### ProblÃ¨me : Pas d'accÃ¨s depuis le navigateur

**VÃ©rifications** :
1. `/etc/hosts` configurÃ© ?
   ```bash
   cat /etc/hosts | grep devboard
   ```
2. Ingress crÃ©Ã©s ?
   ```bash
   kubectl get ingress -A
   ```
3. Test curl :
   ```bash
   curl -H "Host: grafana.devboard.local" http://192.168.1.40
   ```

---

## 6. Maintenance

### Mettre Ã  jour K3s

```bash
# Sur le server
ssh root@192.168.1.40
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.XX.X+k3s1 sh -

# Sur les agents
ssh root@192.168.1.41
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.XX.X+k3s1 K3S_URL=https://192.168.1.40:6443 K3S_TOKEN=<token> sh -
```

### Sauvegarder le cluster

```bash
# Sauvegarder etcd (donnÃ©es K3s)
ssh root@192.168.1.40
k3s etcd-snapshot save

# Sauvegarder les manifests
kubectl get all -A -o yaml > backup-k8s-resources.yaml
```

### RedÃ©marrer les services

```bash
# RedÃ©marrer K3s
ssh root@192.168.1.40 "systemctl restart k3s"

# RedÃ©marrer un deployment
kubectl rollout restart deployment <name> -n <namespace>
```

---

## 7. Commandes utiles

```bash
# Ã‰tat du cluster
kubectl get nodes
kubectl get pods -A
kubectl top nodes
kubectl top pods -A

# Logs d'un service K3s
ssh root@192.168.1.40 "journalctl -u k3s.service -f"

# Kubeconfig
export KUBECONFIG=/home/tom/Dev/projet-etude/infra/ansible/kubeconfig.yaml

# Helm
helm list -A
helm upgrade --install <release> <chart> -f values.yaml -n <namespace>

# Images dans K3s
ssh root@192.168.1.40 "k3s ctr images ls"
```

---

## ğŸ“š RÃ©fÃ©rences

- [K3s Documentation](https://docs.k3s.io/)
- [Traefik Kubernetes Ingress](https://doc.traefik.io/traefik/providers/kubernetes-ingress/)
- [Proxmox LXC](https://pve.proxmox.com/wiki/Linux_Container)
- [Ansible Documentation](https://docs.ansible.com/)
